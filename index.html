<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Lecture 9</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
	<link rel="stylesheet" href="css/normalize.css">
	<link rel="stylesheet" href="css/skeleton.css">
</head>
<body>
	<div class="container">
		<div class="row">
			<div class="twelve columns" style="margin-top: 10%; margin-bottom: 15%">
				<h1>Lecture 9 Scribe Notes</h1>
				<h5>CS 111 at UCLA | Winter 2016</h5>
				<hr>
				<h4>Goldilocks</h4>
				<p>In this lecture, we mostly talked about mechanisms for dealing with and avoiding race conditions. This is done with a <i>lock</i>. A lock is a variable that tells the machine whether or not it is allowed to execute a section of code. If one thread locks a section of code, other threads will not be able to enter that section of code until the first thread has unlocked it. These lockable sections of code are called <i>critical sections</i>. There are two important things to keep in mind when dealing with critical sections. (Well, there are a lot more than two, but we'll just focus on these two for now.)</p>
				<ol style="padding-left: 5%; padding-right: 5%">
					<li>It is important that these critical sections not be too large, or they will slow down your program to an unnecessary degree.</li>
					<li>It is important that these critical sections not be too small, or they will not include all of the code that should be locked and introduce race conditions.</li>
				</ol>
				<p>This need for balance is known as the <i>Goldilocks Principle</i>.</p>
				<p>Consider this example: We have a program that manages bank accounts and has a function <code>transfer(from, to)</code> to make transfers between accounts. Suppose we implement this function in the following way.</p>
				<pre><code>void transfer(account* from, account* to, int amount) {
  from->balance -= amount;
  to->balance += amount;
}</code></pre>
				<p>Can you see the problem here? It should be obvious to you; if we are transferring money to account A at the same time as someone else, we will run into trouble. A quick solution to this problem would be to wrap the actions on the accounts in locks so that only one thread can change be changing an account's balance at a time:</p>
				<pre><code>void transfer(account* from, account* to, int amount) {
  lock(from);
    from->balance -= amount;
  unlock(from);
  lock(to);
    to->balance += amount;
  unlock(to);
}</code></pre>
				<p>This solution helps a bit, but it doesn't solve all of our problems. There is still a period, between the two locks, where we are not in a totally valid state. This is an example of critical sections that are too small. To solve this, we will wrap both of the actions at the same time:</p>
				<pre><code>void transfer(account* from, account* to, int amount) {
  lock(from);
    lock(to);
      from->balance -= amount;
      to->balance += amount;
    unlock(to);
  unlock(from);
}</code></pre>
				<p>Alright, now it looks like we're in better shape, but unfortunately we still have some problems. We're holding two locks at the same time. What happens when we've got a lock on account A and we are about to lock account B as well, but another thread has already locked B and is waiting for A? This situation is called <i>deadlock</i>. Following the goldilocks principle has the potential to create deadlock like this.</p>
				
				<h4>Critical Section Implementation</h4>
					<h5>On a single processor</h5>
						<p>We want our critical section to small enough so that it doesn't interfere with performance, but big enough so that it doesn't introduce races in our code. On a single processor, this isn't difficult, because there aren't any other cores or threads that we are competing with. Using the bank analogy example, consider when we want to deposit money into our account. This might look something like this:</p>
						<pre><code>balance += amt; </code></pre>
						<p>But an increment is actually a couple of instructions in machine language (we have to load the balance, increment it, and store it), so we need to make sure that all of these instructions complete and are not interrupted in the middle. On a single CPU machine, we can do it by:</p>
						<ol style="padding-left: 5%; padding-right: 5%">	
							<li>Disabling interrupts temporarily - if we disable the clock interrupt on our machine, the kernel won't give the CPU to some other thread and we can complete this task before we switch threads. 
							<li>Cooperative multitasking - we execute no instructions when entering or exiting the criical section and ensure that the CPU is not given up in the middle of a critical section. 
						</ol> 
					<h5>On multiple processors</h5>
						<p>With multiple processors, races between threads become more prevalent since multiple cores are executing code at the same time. Consider an implementation of pipes from scratch, without using the linux system call:</p>
						<pre><code>struct pipe {
	char buf[1024]; //arbitrary length
	unsigned r, w; //read and write pointers that indicate where in the buffer we should read or write from
} </code></pre>
						<p>Assume that the buffer is circular, so that when a read or write pointer hits the end of the buffer it continues from the beginning. However, this code is not entirely correct, because we can't tell an empty pipe from a full one. So, instead of accessing r and w directly as indexes into the buffer (where their values would range from 0 to 1023), we make their values between 0 and UINT_MAX. Then if a pointer r or w % 1024 is 0, the pipe is empty, and if it is 1 (or higher), the pipe is full. The following code will tell us how many spots are in the buffer:</p>
						<pre><code>(w - r) % 2014)) </pre></code> 
						<h6>NOTE TO SELF: USE DIAGRAMS TO EXPLAIN WHATS GOING ON WITH THE BUFFER AND POINTERS HERE. CHECK 38:00 and 39:23 REMAINING IN THE VIDEO</h6>	
						<p>Now, we have to create primitives that can access the pipe. To keep it simple, we force reads and writes to be 1 byte at a time. </p>
						<pre><code>bool write_pipe(struct pipe *p, char c) {
	if (p->w - p->r == 1024) //check to make sure the pipe isn't full. 
		return false; 
	//will increment write pointer and mod by 1024 so we can index the buf appropriately
	p->buf[p->w++%1024] = c; 
	return true; 
} </code></pre>
						<pre><code>int read_pipe(struct pipe *p) {
	//if the pipe is empty, return an int that can't represented so the caller knows its an error. 
	if (p->w == p->r)
		return CHAR_MIN - 1; 
	return p->buf[p->r++%1024];
}</code></pre>
						<p>On a threaded system, though, concurrency introduces the problem of scheduling reads and writes. Consider the problem of Thread A is trying to write to the pipe at the same time that Thread B is reading from it. We can fix this by using spin locks, so that if one thread has the lock, the other threads will have to wait to edit the pipe until that thread releases the lock.</p> 
						<pre><code>bool write_pipe(struct pipe *p, char c) {
	lock(&p->l);
	bool full = p->w - p->r == 1024;
	if (!full)
		p->buf[p->w++%1024] = c;
	unlock(&p->l);
	return !full; 
}</code></pre>
						<p></p>
						<pre><code>void read_pipe(struct pipe *p) {
	lock(&p->l);
	char c;
	if (p->r == p->w) 
		c = p->buf[p->r++%1024];
	else
		c = CHAR_MIN - 1;
	unlock(&p->l);
	return c;
}</code></pre>
						<p>But how do we actually implement locks?</p>
				<h4>Lock Implementation</h4>
						<p>We can implement locks as either system calls or as plain machine code.</p>
						<h5>System Calls</h5>
							<p>The kernel will trap each time you want to acquire a lock or release a lock, figure out what other threads are interested in that same lock, and arbitrate. It will let you release your lock and grant it to another thread that wants it, and it will let you acquire a lock if possible. 
							<p>PROS: Easy to implement</p>
							<p>CONS: Slow because of the numerous times we have to switch between user and kernel mode</p>
						<h5>Regular Code</h5>
							<p>We can implement locks using simple locking and unlocking methods:</p>
							<pre><code>//precondition: we don't have a lock
//postcondition: we have a lock
//convention: l = 0 is unlocked, and l = 1 is locked
typedef char lock_t;
void lock (lock_t *l) {
	while (*l)
		continue;
	*l = 1;
	return; 
}</code></pre>
							<p>But we now have to worry about races, because this is just ordinary code that any thread can execute in user mode. There's a race condition right after the while ends, because some other thread might come out of the loop at the same exact time we do, both will grab the lock and set l to 1, and later down the road we'll get a deadlock. We get out of this by asking the hardware people to help us out.</p>    
							<p>PROS: Harder, because of race conditions described above.</p>
							<p>CONS: Faster than using an ordinary system call.</p>





						<!--ADD ALL CODE BELOW THIS POINT, AND WITHIN THIS DIV--> 
						<!--ROHAN'S SECTION:-->





			</div>
		</div>
	</div>
</body>
</html>